+ 现在就是在真实的环境里，一次完整的交互，其实不是那么的容易得到，一次怎么才能做到 不断进化的进行情感识别呢？？
+ 某些， 决策部分的神经元 怎么加进去
+ 用最简单的想法，就是每次成功的交互，会把此时的输入， 给进去，然后放在，交互结果对应的buffer中
+ 就用穿红色衣服去摸它吗？？ 慢慢的红色的脉冲输出就会被统计在 开心的buffer中， 所以就ok了？
  + 既要强化正确的buffer 还要弱化错误的buffer

+ 考虑到短期记忆和长期记忆的话



==============================================================================================

+ 输出的话， 可以使用比例做一个数据的强度，不只有一个分类的结果

+ 输入的话，如果是在线学习的话，可以把每一次成功的交互的过去的多个时间获取的输入，取个平均， 暂时是累计

+ 相当于准备一个线程，专门用来统计输入， 长度是5， 不断更新掉老的，每次成功交互后进行一次平均的归类

==============================================================================================

这两个类其实还挺ok的

==============================================================================================

+ 今天再拍一个 酒精的视频， 再把在线学习的逻辑跑一遍

==============================================================================================
+ Todo：
  + 把情感 到动作的那两个线程 再捋顺一点， 要方便实验

  + 大致思路是 红衣服过去，狗做一次下蹲
  + 红衣服过去 摸， 还是下蹲
  + 摸了一定次数 开始摇头

+ Todo：
  + 现在似乎可以达到了希望的效果，但是如果深入进去会发现，由于我预训练的原因，导致某一种情感的神经元全都具有所有的输入特征
  + 这显然是不合理的，因此可能要重新预训练，或者是输入完全分离预训练，或者是 引入一定的比例预训练
  + 先拍个视频，问问专利的事

+ 使用 完全独立的输入分离预训练后的single test 可以发现很明显的一点就是，数量少的情感输入（比如 抚摸我只给了一个），的脉冲发放时刻要更靠后我这里2.6s，
+ 这里理解起来也就是 输入强度低， 因此需要基类更久的信号才能脉冲
+ 而红色输入，我这里给了两个输入，就会有更快的脉冲 1.2s
+ 因此结果就是 当他们混合输入的时候， 先脉冲的会将后脉冲的 抑制住，这么弄的话， 我的负面抑制buffer 不久不起作用了吗
+ 或者说这里 的抚摸 真的不该当作输入参数吗？？？？？ 就完全当作 交互 的判定信号

+ 我再测试一下 综合输入的情况
+ 发现还是差不多，抚摸和红色同时输入时，抚摸基本是碾压的， 因此其实抚摸最好还是作为和指令交互一样的 进行交互判定的输入，而不是状态输入

+ Todo：
  + 把imu 改成 交互标志位的情况下，还需要再测试一下
  + 调试完成, 这种正向修正, 反向修正, 正向强化的调节机制要比之前顺畅得多
  + 发现一个有意思的点，就是由于前向传播后面还有一层的连接，因此我目前的侧向抑制 往往留有了一个时间步的间隙， 也就相当于留有了 一个其他神经元的脉冲空间
  + 不知道， 会不会有用

+ Todo：
  + 看看相关论文， 尝试手动编译工具
  + 将stdp和特殊lif放到 darwin3上应该还是挺麻烦的 感觉可能得慢慢来
  + 先用spaic实现一个超级简单的分类任务，然后把这个分类任务编译出来试一下， 行的化，再去尝试stdp 的神经元和学习方式
  + 先不使用嵌套，只使用L2作为解码层



  + 输入 要怎么想办法 丰富一下
  + 输出 要怎么多样化一点
  + 评价指标，图像
  + 和强化学习做对比还是挺重要的
  + 如果手动工具要做很久的话，就先把别的内容写出来吧，需要工作量的内容，早晚都要做

