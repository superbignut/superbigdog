+ 现在就是在真实的环境里，一次完整的交互，其实不是那么的容易得到，一次怎么才能做到 不断进化的进行情感识别呢？？
+ 某些， 决策部分的神经元 怎么加进去
+ 用最简单的想法，就是每次成功的交互，会把此时的输入， 给进去，然后放在，交互结果对应的buffer中
+ 就用穿红色衣服去摸它吗？？ 慢慢的红色的脉冲输出就会被统计在 开心的buffer中， 所以就ok了？
  + 既要强化正确的buffer 还要弱化错误的buffer

+ 考虑到短期记忆和长期记忆的话



==============================================================================================

+ 输出的话， 可以使用比例做一个数据的强度，不只有一个分类的结果

+ 输入的话，如果是在线学习的话，可以把每一次成功的交互的过去的多个时间获取的输入，取个平均， 暂时是累计

+ 相当于准备一个线程，专门用来统计输入， 长度是5， 不断更新掉老的，每次成功交互后进行一次平均的归类

==============================================================================================

这两个类其实还挺ok的

==============================================================================================

+ 今天再拍一个 酒精的视频， 再把在线学习的逻辑跑一遍

==============================================================================================
+ Todo：
  + 把情感 到动作的那两个线程 再捋顺一点， 要方便实验

  + 大致思路是 红衣服过去，狗做一次下蹲
  + 红衣服过去 摸， 还是下蹲
  + 摸了一定次数 开始摇头

+ Todo：
  + 现在似乎可以达到了希望的效果，但是如果深入进去会发现，由于我预训练的原因，导致某一种情感的神经元全都具有所有的输入特征
  + 这显然是不合理的，因此可能要重新预训练，或者是输入完全分离预训练，或者是 引入一定的比例预训练
  + 先拍个视频，问问专利的事

+ 使用 完全独立的输入分离预训练后的single test 可以发现很明显的一点就是，数量少的情感输入（比如 抚摸我只给了一个），的脉冲发放时刻要更靠后我这里2.6s，
+ 这里理解起来也就是 输入强度低， 因此需要基类更久的信号才能脉冲
+ 而红色输入，我这里给了两个输入，就会有更快的脉冲 1.2s
+ 因此结果就是 当他们混合输入的时候， 先脉冲的会将后脉冲的 抑制住，这么弄的话， 我的负面抑制buffer 不久不起作用了吗
+ 或者说这里 的抚摸 真的不该当作输入参数吗？？？？？ 就完全当作 交互 的判定信号

+ 我再测试一下 综合输入的情况
+ 发现还是差不多，抚摸和红色同时输入时，抚摸基本是碾压的， 因此其实抚摸最好还是作为和指令交互一样的 进行交互判定的输入，而不是状态输入

+ Todo：
  + 把imu 改成 交互标志位的情况下，还需要再测试一下
  + 调试完成, 这种正向修正, 反向修正, 正向强化的调节机制要比之前顺畅得多
  + 发现一个有意思的点，就是由于前向传播后面还有一层的连接，因此我目前的侧向抑制 往往留有了一个时间步的间隙， 也就相当于留有了 一个其他神经元的脉冲空间
  + 不知道， 会不会有用

+ Todo：
  + 看看相关论文， 尝试手动编译工具
  + 将stdp和特殊lif放到 darwin3上应该还是挺麻烦的 感觉可能得慢慢来
  + 先用spaic实现一个超级简单的分类任务，然后把这个分类任务编译出来试一下， 行的化，再去尝试stdp 的神经元和学习方式
  + 先不使用嵌套，只使用L2作为解码层



  + 输入 要怎么想办法 丰富一下
  + 输出 要怎么多样化一点
  + 评价指标，图像
  + 和强化学习做对比还是挺重要的
  + 如果手动工具要做很久的话，就先把别的内容写出来吧，需要工作量的内容，早晚都要做
  + 接下来就是看自定义神经元的实现
+ spaic 的 stdp权重更新方式还挺有意思的， 输出脉冲* 输入trace - 输入脉冲*输出trace 
  + 细细品味，确实很有stdp的味道， 但是还不知道出处是哪里
---
+ darwin3 有 26 个控制寄存器， 6个状态寄存器，16个运行寄存器
  + load 把存储器读到寄存器
  + store 把寄存器读到存储器
  + 问一下前几个汇编的含义 和 自定义神经元
  + 问一下树突轴突
  + 问一下，如果要更新权重 的 NPC 函数
  + R3寄存器直接用的吗？？

+ 如果权重还不支持学习的话，想一想其他方法，或者有哪些是可以修改的，比如阈值？，比如我外部的buffer
+ 现在看起来的话，似乎可以把不同神经元 config 放到 不同的核心上
+ 现在就想着怎么操作一下， 也就是这两个lif 的量化的问题， 先试一下 alif， 把所有的值都改成正数
+ lifex 行的话， lif ih也没问题

+ 先试一下，把衰减阈值去掉之后，stdp是否能正确训练？进而验证出阈值衰减的必要性？？？
+ 在手写体里面，阈值的变化确实很夸张，原本是-52，一顿强化之后竟然达到了 -27， 接近一半 才触发下一个模式的脉冲
+ 每次增长0.05 的 vth_theta 增长到了 1.9
+ 有一个疑问，每次spaic input 输入的时候 神经元的电压会进行重新的 更新吗
+ 在darwin上，有个地方需要注意 就是 我的模电压 25个时间 后 会重新 赋值一次，类似于 spaic中的 下一个timestep
  + 这里感觉 得用 jmp 来做，就是当到了 25 的倍数的时候，就要对某些变量清零一次然后加载回 存储器
+ 而 vth_theta 似乎才是 最正统的每次都要 保存，测试了一下感觉没问题？？ 但是阈值和电压是负数 在darwin3 上可能还要在测试一下 Todo


+ 先测试 重置阈值正不正确                 --0   重置阈值是ok的
+ 所以接下来的话，先测试，负数阈值能不能做  --1   电压阈值设置成负数也是ok的
+ 再测试写一个带vth_theta 的 lif          --2   用jmp 搞定了简单的阈值自适应
+ 再用jmp 一定时间步进行清空               --3  Todo   只有电压需要清空
+ stdpex stdpih 分别测试                  --4  就是LIF
+ 连接起来测试                           --5   Todo

---


+ 现在的话就是去实现，stdp更新input到layer1层的连接

+ LSLS 是存取学习状态
+ UPTLS 是 用来更新 四个输入、输出迹 的公式， 更新学习状态
+ LSSYN 存取权重
+ UPTSYN 

PRT1A-X PRT1A-Y  输入脉冲迹

PRT1B-X PRT1B-Y 输出脉冲迹

PRT0-X PRT0-Y 奖惩参数，也就是常量

PRTR 

WPAR0 - WPAR15


问题，参数更新过程，比如权重和的更新是在哪里进行的呢， 那么迹的更新是在哪里进行的呢？

手册里指到的 "脉冲迹"， 是脉冲次数的和？还是什么定义？



+ 先配置好 lpar0-lpar6 作为trace 计算的参数  WPAR0 - WPAR1516个寄存器作为权重更新参数


LSLS 加载所有迹 1Ax 1Ay 1bx 1by 0x 0y  (0r 1r) 
UPTLS 使用迹 和 lpar 寄存器的参数计算 得到新的 1Ax 1Ay 1bx 1by r  这里相当于对迹进行了一些操作
LSLS 把迹存回去

LSSYN 读取权重 到权重寄存器
UPTSYN 更新权重
LSSYN 把权重存回去

---


怎么优化一下dmx 的输入和输出

---


+ 先解决2.4版本的runtime问题 是文件缺失的原因 

+ 每次都重启，要不然不干净


+ 学习 npc 调试ok 要进一步尝试把代码放进去了
  + 试一下三个npc里面的跳转
  + 其余的再说吧，先放假喽

+ 查到了有256 个树突 每个树突100个连接，但是 我只给一个输入层神经元有输入， 也就应该是 一个树突 有输入脉冲， 但怎么有两个 位置 是8000 呢


---


问题 1 ：
  + 怎么查看所有的学习状态存储器（迹）
  + UPTSYN todo 写的对不对
  + 权重怎么读出来， 如果不能读出来，权重就没办法可视化
  + 为什么要有奇偶时间步呢

Todo:
  + 迹的衰减 可能要处理一下，
  + 权重读取与可视化
  + ppt 中的那个 迹的衰减是怎么做出来的呢

+ 迹的衰减可以自动进行
+ 学习状态寄存器 reset 重置一下


Debug:
  + 迹测试的时候[0 1 0 0 0 0 0 0 0 0 ] 这么写不对，一个 0 就代表一个输入了， 如果是第三个  应该写成 [[3]], 就ok 了要不测试总出现 dfff 和 dffe 是 8000 真服了奥

---

Todo:
  + 移位实现迹的衰减 08配置寄存器 写不到, 手动改写
  + 每个大周期 要清空 trace 吗 ： 要！

Todo:
  + 权重已经能读出来了，现在的话，要用spaic-stdp 的同样的方式 plot 一下，方便之后 做对比， 观察训练的效果